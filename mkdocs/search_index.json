{
    "docs": [
        {
            "location": "/", 
            "text": "Serps - Home\n\n\nThe PHP Search Engine Result Page Scraper\n\n\n\n\nWeb scraping\n (web harvesting or web data extraction) is a computer software technique of extracting \ninformation from websites. Usually, such software programs simulate human exploration of the World Wide Web \nby either implementing low-level Hypertext Transfer Protocol (HTTP), \nor embedding a fully-fledged web browser, such as Mozilla Firefox.\n\n\nWikipedia\n\n\n\n\nOverview\n\n\nSerps is a set of tools that make it easy to parse \npopular search engines\n and to create reports.\nIt helps to parse \nSERP\n (Search Engine Result Page) and gives you a standard output of what is parsed.\n\n\nThe problem\n\n\nMost of times search engines don't want you to parse them, and they don't offer a documentation or a standard way \nto extract the results from the SERP.\n\n\nWe tried to solve this problems by \nanalysing\n how search engines behave and we give you the necessary tools to\nwork with them, from the URL generation to the parsing of the results. \nAt the endpoint we give you a \nstandard and documented API\n.\n\n\nTODO\n We place additional efforts on monitoring them to make sure that our tools are always up to date.\n\n\nGetting Started\n\n\nLooking forward to work with the library? \n\n\n\n\nRead our \ngetting started\n.\n\n\nCheck the \navailable search engines\n.\n\n\nGet information about \nproxies\n and \nhttp client implementations\n.\n\n\nWork with \ncaptcha services\n.\n\n\n\n\nLicensing\n\n\nThe work is placed under the terms of the \nFair License\n.\n\n\n\n\nUsage of the works is permitted provided that this instrument is retained with the works, \nso that any entity that uses the works is notified of this instrument.\n\n\nDISCLAIMER: THE WORKS ARE WITHOUT WARRANTY.", 
            "title": "Home"
        }, 
        {
            "location": "/#serps-home", 
            "text": "The PHP Search Engine Result Page Scraper   Web scraping  (web harvesting or web data extraction) is a computer software technique of extracting \ninformation from websites. Usually, such software programs simulate human exploration of the World Wide Web \nby either implementing low-level Hypertext Transfer Protocol (HTTP), \nor embedding a fully-fledged web browser, such as Mozilla Firefox.  Wikipedia", 
            "title": "Serps - Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Serps is a set of tools that make it easy to parse  popular search engines  and to create reports.\nIt helps to parse  SERP  (Search Engine Result Page) and gives you a standard output of what is parsed.", 
            "title": "Overview"
        }, 
        {
            "location": "/#the-problem", 
            "text": "Most of times search engines don't want you to parse them, and they don't offer a documentation or a standard way \nto extract the results from the SERP.  We tried to solve this problems by  analysing  how search engines behave and we give you the necessary tools to\nwork with them, from the URL generation to the parsing of the results. \nAt the endpoint we give you a  standard and documented API .  TODO  We place additional efforts on monitoring them to make sure that our tools are always up to date.", 
            "title": "The problem"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Looking forward to work with the library?    Read our  getting started .  Check the  available search engines .  Get information about  proxies  and  http client implementations .  Work with  captcha services .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#licensing", 
            "text": "The work is placed under the terms of the  Fair License .   Usage of the works is permitted provided that this instrument is retained with the works, \nso that any entity that uses the works is notified of this instrument.  DISCLAIMER: THE WORKS ARE WITHOUT WARRANTY.", 
            "title": "Licensing"
        }, 
        {
            "location": "/getting-started/", 
            "text": "Getting Started\n\n\nThis getting started will help you to understand how the library is built and what are its main components.\n\n\nInstall\n\n\nTwo work with SERPS you need two things:\n\n\n\n\nOne or more search engine clients you want to parse\n\n\nAn http clients\n\n\n\n\nComposer\n is required to manage the necessary dependencies.\n\n\n\n\nExample with the \nGoogle client\n and the \nCurl http client\n\n\n\n\n{\n    \nrequire\n: {\n        \nserps/search-engine-google\n: \n*\n,\n        \nserps/http-client-curl\n: \n*\n\n    }\n}\n\n\n\n\nSearch Engine client\n\n\nIn a regular workflow a search engine client allows to:\n\n\n\n\nManipulate an url and generate a request specific to the search engine\n\n\nRetrieve the response from the search engine\n\n\nParse this response to a standard sets of results\n\n\n\n\nEach \nsearch engine\n has its set of specificities and thus each search engine implementation has its own dedicated guide.\n\n\nCheck the list of \navailable search engines\n.\n\n\nHttp client\n\n\nWorking with search engines involves to work with \nhttp requests\n to be able to interact with them.\nUssually the \nsearch engine client\n will need a http client to work correctly.\n\n\n\n\nExample with the \ngoogle client\n and the \ncurl http client\n\n\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n\n\n\nCheck the list of \navailable http clients\n.\n\n\nCaptcha\n\n\nAs said before, most of time search engines don't want you to parse them, additionally if you submit a lot of \nrequests to them, they might - \nthey will\n - detect you as a bot and they will send you a \ncaptcha\n that you have\nto solve before you continue.\n\n\nOf course captcha are search engine dependent, but we are aware of this issue\nand we offer you a common interface to solve them. \n\n\n\n\nExample of \ncaptcha handling\ns with the google client\n\n\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\Exception\\CaptchaException;\n    use Serps\\Exception\\CaptchaException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    try{\n        $googleClient-\nquery($googleUrl, $proxy);\n    }catch(CaptchaException $e){  \n        $captcha = $e-\ngetCaptcha();\n    }\n\n\n\n\nRead more about \ncaptcha\n.\n\n\nProxies\n\n\nWhen you deal with a very \nlarge number of requests\n solving captcha is not enough, you will need to send requests\nthrough proxies.\n\n\nThis is a major feature of scraping and we placed proxies at the very heart of the library. \nWe made the choice to make each request being proxy aware. \nThis way with a single client you can use as many proxies as you want.\n\n\n\n\nExample of \nproxy\n usage with the google client\n\n\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleClient-\nquery($googleUrl, $proxy);\n\n\n\n\nRead more about \nproxies\n.\n\n\nProxies and captcha\n\n\nWhen a proxy got blocked by a captcha it's important to solve the captcha with the proxy, if you don't, the search\nengine might detect it's not the same ip that solved the captcha and wont accept to solve it.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting-started/#getting-started", 
            "text": "This getting started will help you to understand how the library is built and what are its main components.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting-started/#install", 
            "text": "Two work with SERPS you need two things:   One or more search engine clients you want to parse  An http clients   Composer  is required to manage the necessary dependencies.   Example with the  Google client  and the  Curl http client   {\n     require : {\n         serps/search-engine-google :  * ,\n         serps/http-client-curl :  * \n    }\n}", 
            "title": "Install"
        }, 
        {
            "location": "/getting-started/#search-engine-client", 
            "text": "In a regular workflow a search engine client allows to:   Manipulate an url and generate a request specific to the search engine  Retrieve the response from the search engine  Parse this response to a standard sets of results   Each  search engine  has its set of specificities and thus each search engine implementation has its own dedicated guide.  Check the list of  available search engines .", 
            "title": "Search Engine client"
        }, 
        {
            "location": "/getting-started/#http-client", 
            "text": "Working with search engines involves to work with  http requests  to be able to interact with them.\nUssually the  search engine client  will need a http client to work correctly.   Example with the  google client  and the  curl http client       use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());  Check the list of  available http clients .", 
            "title": "Http client"
        }, 
        {
            "location": "/getting-started/#captcha", 
            "text": "As said before, most of time search engines don't want you to parse them, additionally if you submit a lot of \nrequests to them, they might -  they will  - detect you as a bot and they will send you a  captcha  that you have\nto solve before you continue.  Of course captcha are search engine dependent, but we are aware of this issue\nand we offer you a common interface to solve them.    Example of  captcha handling s with the google client       use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\Exception\\CaptchaException;\n    use Serps\\Exception\\CaptchaException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    try{\n        $googleClient- query($googleUrl, $proxy);\n    }catch(CaptchaException $e){  \n        $captcha = $e- getCaptcha();\n    }  Read more about  captcha .", 
            "title": "Captcha"
        }, 
        {
            "location": "/getting-started/#proxies", 
            "text": "When you deal with a very  large number of requests  solving captcha is not enough, you will need to send requests\nthrough proxies.  This is a major feature of scraping and we placed proxies at the very heart of the library. \nWe made the choice to make each request being proxy aware. \nThis way with a single client you can use as many proxies as you want.   Example of  proxy  usage with the google client       use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleClient- query($googleUrl, $proxy);  Read more about  proxies .", 
            "title": "Proxies"
        }, 
        {
            "location": "/getting-started/#proxies-and-captcha", 
            "text": "When a proxy got blocked by a captcha it's important to solve the captcha with the proxy, if you don't, the search\nengine might detect it's not the same ip that solved the captcha and wont accept to solve it.", 
            "title": "Proxies and captcha"
        }, 
        {
            "location": "/search-engine/google/", 
            "text": "Google Client\n\n\nInstallation\n\n\nThe google client is available with the package \n\nserps/search-engine-google\n: \n\n\n$ composer require 'serps/search-engine-google'\n\n\nOverview\n\n\nThe google client needs an http client interface to be constructed and an url to be parsed\n\n\n\n\nOverview of querying google for the keyword 'simpsons' and getting the natural results\n\n\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $google-\nsetSearchTerm('simpsons');\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        $resultTitle = $result-\ngetDataValue('title');\n    }\n\n\n\n\nTODO", 
            "title": "Google"
        }, 
        {
            "location": "/search-engine/google/#google-client", 
            "text": "", 
            "title": "Google Client"
        }, 
        {
            "location": "/search-engine/google/#installation", 
            "text": "The google client is available with the package  serps/search-engine-google :   $ composer require 'serps/search-engine-google'", 
            "title": "Installation"
        }, 
        {
            "location": "/search-engine/google/#overview", 
            "text": "The google client needs an http client interface to be constructed and an url to be parsed   Overview of querying google for the keyword 'simpsons' and getting the natural results       use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $google- setSearchTerm('simpsons');\n\n    $response = $googleClient- query($googleUrl);\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        $resultTitle = $result- getDataValue('title');\n    }  TODO", 
            "title": "Overview"
        }
    ]
}